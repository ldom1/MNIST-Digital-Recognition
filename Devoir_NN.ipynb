{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Digital Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subject today is digit recognition using neural networks. More precisely, the task is to train a neural network so that it finds the digit in a given image of handwritten digit.\n",
    "The first step is to split the MNIST dataset into a training set, a validation set, and a test set. During the training phase, the neural network is trained on the training set. Still during the training phase, the recognition score of the neural network is checked on a subset of the training set and on the validation set in order to detect potential overfitting. During the test set, the true classification score of the fully-connected neural network is computed on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed packages\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "import tools.tools as tls\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Process the data and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/mnist/data/'\n",
    "\n",
    "train_img = np.load(path+'training_images.npy')\n",
    "train_labels = np.load(path+'training_labels.npy')\n",
    "\n",
    "valid_img = np.load(path+'validation_images.npy')\n",
    "valid_labels = np.load(path+'validation_labels.npy')\n",
    "\n",
    "test_img = np.load(path+'test_images.npy')\n",
    "test_labels = np.load(path+'test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img\n",
    "train_img_new = train_img.reshape(train_img.shape[0],\n",
    "                                  train_img.shape[1]**2).astype('uint64')\n",
    "valid_img_new = valid_img.reshape(valid_img.shape[0],\n",
    "                                  valid_img.shape[1]**2).astype('uint64')\n",
    "test_img_new = test_img.reshape(test_img.shape[0],\n",
    "                                test_img.shape[1]**2).astype('uint64')\n",
    "\n",
    "# label\n",
    "train_lab_new = np.array([np.argmax(train_labels[y])\n",
    "                          for y in range(len(train_labels))])\n",
    "valid_lab_new = np.array([np.argmax(valid_labels[y])\n",
    "                          for y in range(len(valid_labels))])\n",
    "test_lab_new = np.array([np.argmax(valid_labels[y])\n",
    "                         for y in range(len(valid_labels))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random permutation of the train set (for the training phase of the neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut_index = np.random.permutation(train_img_new.shape[0])\n",
    "\n",
    "train_img = train_img[permut_index, :, :]\n",
    "train_img_new = train_img_new[permut_index, :]\n",
    "train_labels = train_labels[permut_index, :]\n",
    "train_lab_new = train_lab_new[permut_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation (exported in the folder 'img_observation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualisation exported\n"
     ]
    }
   ],
   "source": [
    "# Visualisation\n",
    "def visualize(path, name, X, y, nb_img=36):\n",
    "    \"\"\"Allows to vizualise 36 img and the labels\"\"\"\n",
    "    vector_viz = [rd.randint(0, len(X)-1) for y in np.arange(nb_img)]\n",
    "    # Img\n",
    "    nb_vertically = int(np.sqrt(nb_img))\n",
    "    name_img_out = path+'visualize_img' + '_' + str(name) + '.png'\n",
    "    tls.visualize_grayscale_images(X[vector_viz, :, :, :],\n",
    "                                   nb_vertically, name_img_out)\n",
    "\n",
    "    # Labels\n",
    "    name_lab_out = path+'visualize_labels' + '_' + str(name)\n",
    "    labels = train_lab_new[vector_viz]\n",
    "    labels = labels.reshape((nb_vertically, nb_vertically))\n",
    "\n",
    "    file = open(name_lab_out, \"w\")\n",
    "    labels_str = ''\n",
    "    for line in labels:\n",
    "        for elt in line:\n",
    "            labels_str = labels_str + str(elt) + ' '\n",
    "        labels_str = labels_str + '\\n'\n",
    "    file.write(labels_str)\n",
    "    file.close()\n",
    "    print('Visualisation exported')\n",
    "\n",
    "\n",
    "path = os.getcwd() + '/img_observation/'\n",
    "X = train_img\n",
    "y = train_lab_new\n",
    "visualize(path, 'obs', X, y, nb_img=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the model within the file 'NeuralNetwork.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "from NeuralNetwork import neural_net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "nb_images = train_img_new.shape[0]\n",
    "nb_input = train_img_new.shape[1]\n",
    "batch_size = 1000\n",
    "nb_hidden = 16\n",
    "nb_classes = 10\n",
    "learning_rate = 0.5\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used in order to define batches during the training (it is the equivalent of the next batch function of tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_idx(i, batch_size, nb_images):\n",
    "    # Input\n",
    "    max_batch_size = int(nb_images/batch_size)\n",
    "    j = i % max_batch_size\n",
    "    return np.arange(j*batch_size, (j+1)*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "X = tf.placeholder(tf.float32, [None, nb_input])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "p = neural_net_model(X, nb_input, batch_size, nb_hidden, nb_classes,\n",
    "                     learning_rate)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(p),\n",
    "                                              reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(p, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "acc_train = []\n",
    "loss_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:41,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Loss : 0.7647578 Accuracy : 0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:09<00:37,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Loss : 0.6079846 Accuracy : 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:13<00:32,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Loss : 0.575327 Accuracy : 0.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:18<00:27,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Loss : 0.524513 Accuracy : 0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:22<00:22,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Loss : 0.50526345 Accuracy : 0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:27<00:17,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Loss : 0.48668137 Accuracy : 0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:31<00:13,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Loss : 0.4433248 Accuracy : 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:36<00:08,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Loss : 0.43249854 Accuracy : 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:40<00:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Loss : 0.40850416 Accuracy : 0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:44<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Loss : 0.42103118 Accuracy : 0.885\n",
      "\n",
      "Accuracy on test set: 0.8899\n",
      "\n",
      "Errors (%): 0.1101 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        saver.restore(sess, 'mnist_predictions.ckpt')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # train the model mini batch with 100 elements, for 1K times\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            batch_index = get_batch_idx(i, batch_size, nb_images)\n",
    "            batch_x, batch_y = (train_img_new[batch_index, :],\n",
    "                                train_labels[batch_index])\n",
    "            sess.run([train_step], feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "        # Accuracy and cross entropy - Training phase\n",
    "        loss, acc = sess.run([cross_entropy, accuracy],\n",
    "                             feed_dict={X: valid_img_new, Y: valid_labels})\n",
    "        loss_train.append(loss)\n",
    "        acc_train.append(acc)\n",
    "        print(\"\\nEpoch\", str(epoch), \"Loss :\", str(loss),\n",
    "              \"Accuracy :\", str(acc))\n",
    "\n",
    "    # Test\n",
    "    # evaluate the accuracy of the model\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: test_img_new,\n",
    "                                                  Y: test_labels})\n",
    "    print(\"\\nAccuracy on test set:\", test_accuracy)\n",
    "\n",
    "    # pred\n",
    "    pred = sess.run(p, feed_dict={X: test_img_new, Y: test_labels})\n",
    "    pred = np.array([np.argmax(pred[y]) for y in range(len(pred))])\n",
    "    labels = np.array([np.argmax(test_labels[y])\n",
    "                       for y in range(len(test_labels))])\n",
    "\n",
    "    print(\"\\nErrors (%):\", np.sum(labels != pred)/len(pred), '%')\n",
    "\n",
    "    # Save the model\n",
    "    '''if input('Save model ? [Y/N]') == 'Y':\n",
    "        import os\n",
    "        saver.save(sess, os.getcwd() +\n",
    "                   '/nn_saved_sessions/mnist_predictions.ckpt')\n",
    "        print('Model Saved')'''\n",
    "\n",
    "    # Close the session\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), acc_train, label='accuracy', color='blue')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('accuracy - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entrepoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), loss_train, label='loss', color='red')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('loss - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Evolved Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNetwork import neural_net_model_evol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "nb_images = train_img_new.shape[0]\n",
    "nb_input = train_img_new.shape[1]\n",
    "batch_size = 1000\n",
    "nb_hidden1 = 64\n",
    "nb_hidden2 = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 10\n",
    "\n",
    "# Drop out level\n",
    "prob_1 = 0.7\n",
    "prob_2 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used in order to define batches during the training (it is the equivalent of the next batch function of tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_idx(i, batch_size, nb_images):\n",
    "    # Input\n",
    "    max_batch_size = int(nb_images/batch_size)\n",
    "    j = i % max_batch_size\n",
    "    return np.arange(j*batch_size, (j+1)*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "X = tf.placeholder(tf.float32, [None, nb_input])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "keep_prob_1 = tf.placeholder(tf.float32)\n",
    "keep_prob_2 = tf.placeholder(tf.float32)\n",
    "\n",
    "p = neural_net_model_evol(X, nb_input, nb_hidden1, nb_hidden2, nb_classes,\n",
    "                          keep_prob_1, keep_prob_2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(p),\n",
    "                                              reduction_indices=[1]))\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(p, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "learn_rate = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a piecewise constant for learning rate in order not to have a constant value through the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piecewise constant\n",
    "min_lr = 0.1\n",
    "max_lr = 0.5\n",
    "nb_values = 10\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "boundaries = list(np.linspace(batch_size,\n",
    "                              batch_size*nb_epoch, nb_values,\n",
    "                              dtype=np.int32)[:-1])\n",
    "values = list(np.round(np.linspace(max_lr, min_lr, nb_values), 2))\n",
    "learning_rate_pc = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "\n",
    "j = 0\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "learning_step_pc = tf.train.GradientDescentOptimizer(learning_rate_pc).minimize(cross_entropy,\n",
    "                                                                 global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<01:00,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Loss : 0.84792614 Accuracy : 0.747 Learning rate: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:13<00:53,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Loss : 0.64333266 Accuracy : 0.802 Learning rate: 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:20<00:46,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Loss : 0.53148794 Accuracy : 0.842 Learning rate: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:26<00:40,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Loss : 0.48345086 Accuracy : 0.87 Learning rate: 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:33<00:33,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Loss : 0.44860467 Accuracy : 0.871 Learning rate: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:40<00:27,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Loss : 0.42465675 Accuracy : 0.877 Learning rate: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:47<00:20,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Loss : 0.4236121 Accuracy : 0.872 Learning rate: 0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:54<00:13,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Loss : 0.38811955 Accuracy : 0.886 Learning rate: 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [01:00<00:06,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Loss : 0.378854 Accuracy : 0.895 Learning rate: 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [01:07<00:00,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Loss : 0.37489542 Accuracy : 0.893 Learning rate: 0.1\n",
      "\n",
      "Accuracy on test set: 0.8912\n",
      "\n",
      "Errors (%): 10.879999999999999 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        saver.restore(sess, 'mnist_predictions.ckpt')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # train the model mini batch with 100 elements, for 1K times\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Update learning rate\n",
    "            l_rate = sess.run(learning_rate_pc, {global_step: j})\n",
    "            j += 1\n",
    "\n",
    "            batch_index = get_batch_idx(i, batch_size, nb_images)\n",
    "            batch_x, batch_y = (train_img_new[batch_index, :],\n",
    "                                train_labels[batch_index])\n",
    "            sess.run([learning_step_pc], feed_dict={X: batch_x, Y: batch_y,\n",
    "                                                    keep_prob_1: prob_1,\n",
    "                                                    keep_prob_2: prob_2})\n",
    "\n",
    "        # Accuracy and cross entropy - Training phase\n",
    "        loss, acc = sess.run([cross_entropy, accuracy],\n",
    "                             feed_dict={X: valid_img_new, Y: valid_labels,\n",
    "                                        keep_prob_1: 1.0,\n",
    "                                        keep_prob_2: 1.0})\n",
    "        loss_train.append(loss)\n",
    "        acc_train.append(acc)\n",
    "        learn_rate.append(l_rate)\n",
    "        print(\"\\nEpoch\", str(epoch), \"Loss :\", str(loss),\n",
    "              \"Accuracy :\", str(acc), \"Learning rate:\", l_rate)\n",
    "\n",
    "    # Test\n",
    "    # evaluate the accuracy of the model\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: test_img_new,\n",
    "                                                  Y: test_labels,\n",
    "                                                  keep_prob_1: 1.0,\n",
    "                                                  keep_prob_2: 1.0})\n",
    "    print(\"\\nAccuracy on test set:\", test_accuracy)\n",
    "\n",
    "    # pred\n",
    "    pred = sess.run(p, feed_dict={X: test_img_new, Y: test_labels,\n",
    "                                  keep_prob_1: 1.0, keep_prob_2: 1.0})\n",
    "    pred = np.array([np.argmax(pred[y]) for y in range(len(pred))])\n",
    "    labels = np.array([np.argmax(test_labels[y])\n",
    "                       for y in range(len(test_labels))])\n",
    "\n",
    "    print(\"\\nErrors (%):\", np.sum(labels != pred)/len(pred)*100, '%')\n",
    "\n",
    "    # Save the model\n",
    "    '''if input('Save model ? [Y/N]') == 'Y':\n",
    "        import os\n",
    "        saver.save(sess, os.getcwd() +\n",
    "                   '/nn_saved_sessions/mnist_predictions.ckpt')\n",
    "        print('Model Saved')'''\n",
    "\n",
    "    # Close the session\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), acc_train, label='accuracy', color='blue')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('accuracy - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), loss_train, label='loss', color='red')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('loss - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the learning rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), learn_rate, label='learning_rate', color='green')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('learning rate - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now use a exponential decay for the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = []\n",
    "loss_train = []\n",
    "learn_rate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential decay of the learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.7\n",
    "learning_rate_ed = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                              global_step, batch_size, 0.69,\n",
    "                                              staircase=True)\n",
    "j = 0\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "learning_step_ed = tf.train.GradientDescentOptimizer(learning_rate_ed).minimize(cross_entropy,\n",
    "                                                   global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<01:00,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Loss : 0.7083188 Accuracy : 0.786 Learning rate: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:13<00:53,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Loss : 0.5790196 Accuracy : 0.816 Learning rate: 0.48299998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:19<00:46,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Loss : 0.49766472 Accuracy : 0.853 Learning rate: 0.33326998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:26<00:39,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Loss : 0.49356148 Accuracy : 0.863 Learning rate: 0.2299563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:33<00:33,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Loss : 0.47742578 Accuracy : 0.862 Learning rate: 0.15866984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:39<00:26,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Loss : 0.47441584 Accuracy : 0.865 Learning rate: 0.1094822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:46<00:19,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Loss : 0.46262097 Accuracy : 0.863 Learning rate: 0.07554271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:52<00:13,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Loss : 0.45251366 Accuracy : 0.87 Learning rate: 0.05212447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:59<00:06,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Loss : 0.44641545 Accuracy : 0.873 Learning rate: 0.035965886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [01:06<00:00,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Loss : 0.4415327 Accuracy : 0.865 Learning rate: 0.02481646\n",
      "\n",
      "Accuracy on test set: 0.8843\n",
      "\n",
      "Errors (%): 11.57 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize variables and session\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        saver.restore(sess, 'mnist_predictions.ckpt')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # train the model mini batch with 100 elements, for 1K times\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Update learning rate\n",
    "            l_rate = sess.run(learning_rate_ed, {global_step: j})\n",
    "            j += 1\n",
    "\n",
    "            batch_index = get_batch_idx(i, batch_size, nb_images)\n",
    "            batch_x, batch_y = (train_img_new[batch_index, :],\n",
    "                                train_labels[batch_index])\n",
    "            sess.run([learning_step_ed], feed_dict={X: batch_x, Y: batch_y,\n",
    "                                                    keep_prob_1: prob_1,\n",
    "                                                    keep_prob_2: prob_2})\n",
    "\n",
    "        # Accuracy and cross entropy - Training phase\n",
    "        loss, acc = sess.run([cross_entropy, accuracy],\n",
    "                             feed_dict={X: valid_img_new, Y: valid_labels,\n",
    "                                        keep_prob_1: 1.0,\n",
    "                                        keep_prob_2: 1.0})\n",
    "        loss_train.append(loss)\n",
    "        acc_train.append(acc)\n",
    "        learn_rate.append(l_rate)\n",
    "        print(\"\\nEpoch\", str(epoch), \"Loss :\", str(loss),\n",
    "              \"Accuracy :\", str(acc), \"Learning rate:\", l_rate)\n",
    "\n",
    "    # Test\n",
    "    # evaluate the accuracy of the model\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: test_img_new,\n",
    "                                                  Y: test_labels,\n",
    "                                                  keep_prob_1: 1.0,\n",
    "                                                  keep_prob_2: 1.0})\n",
    "    print(\"\\nAccuracy on test set:\", test_accuracy)\n",
    "\n",
    "    # pred\n",
    "    pred = sess.run(p, feed_dict={X: test_img_new, Y: test_labels,\n",
    "                                  keep_prob_1: 1.0, keep_prob_2: 1.0})\n",
    "    pred = np.array([np.argmax(pred[y]) for y in range(len(pred))])\n",
    "    labels = np.array([np.argmax(test_labels[y])\n",
    "                       for y in range(len(test_labels))])\n",
    "\n",
    "    print(\"\\nErrors (%):\", np.sum(labels != pred)/len(pred)*100, '%')\n",
    "\n",
    "    # Save the model\n",
    "    '''if input('Save model ? [Y/N]') == 'Y':\n",
    "        import os\n",
    "        saver.save(sess, os.getcwd() +\n",
    "                   '/nn_saved_sessions/mnist_predictions.ckpt')\n",
    "        print('Model Saved')'''\n",
    "\n",
    "    # Close the session\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), acc_train, label='accuracy', color='blue')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('accuracy - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), loss_train, label='loss', color='red')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('loss - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the learning rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), learn_rate, label='learning_rate', color='green')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('learning rate - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
