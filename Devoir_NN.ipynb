{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Digital Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subject today is digit recognition using neural networks. More precisely, the task is to train a neural network so that it finds the digit in a given image of handwritten digit.\n",
    "The first step is to split the MNIST dataset into a training set, a validation set, and a test set. During the training phase, the neural network is trained on the training set. Still during the training phase, the recognition score of the neural network is checked on a subset of the training set and on the validation set in order to detect potential overfitting. During the test set, the true classification score of the fully-connected neural network is computed on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed packages\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import tensorflow as tf\n",
    "import tools.tools as tls\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Process the data and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/mnist/data/'\n",
    "\n",
    "train_img = np.load(path+'training_images.npy')\n",
    "train_labels = np.load(path+'training_labels.npy')\n",
    "\n",
    "valid_img = np.load(path+'validation_images.npy')\n",
    "valid_labels = np.load(path+'validation_labels.npy')\n",
    "\n",
    "test_img = np.load(path+'test_images.npy')\n",
    "test_labels = np.load(path+'test_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img\n",
    "train_img_new = train_img.reshape(train_img.shape[0],\n",
    "                                  train_img.shape[1]**2).astype('uint64')\n",
    "valid_img_new = valid_img.reshape(valid_img.shape[0],\n",
    "                                  valid_img.shape[1]**2).astype('uint64')\n",
    "test_img_new = test_img.reshape(test_img.shape[0],\n",
    "                                test_img.shape[1]**2).astype('uint64')\n",
    "\n",
    "# label\n",
    "train_lab_new = np.array([np.argmax(train_labels[y])\n",
    "                          for y in range(len(train_labels))])\n",
    "valid_lab_new = np.array([np.argmax(valid_labels[y])\n",
    "                          for y in range(len(valid_labels))])\n",
    "test_lab_new = np.array([np.argmax(valid_labels[y])\n",
    "                         for y in range(len(valid_labels))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random permutation of the train set (for the training phase of the neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "permut_index = np.random.permutation(train_img_new.shape[0])\n",
    "\n",
    "train_img = train_img[permut_index, :, :]\n",
    "train_img_new = train_img_new[permut_index, :]\n",
    "train_labels = train_labels[permut_index, :]\n",
    "train_lab_new = train_lab_new[permut_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation (exported in the folder 'img_observation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualisation exported\n"
     ]
    }
   ],
   "source": [
    "# Visualisation\n",
    "def visualize(path, name, X, y, nb_img=36):\n",
    "    \"\"\"Allows to vizualise 36 img and the labels\"\"\"\n",
    "    vector_viz = [rd.randint(0, len(X)-1) for y in np.arange(nb_img)]\n",
    "    # Img\n",
    "    nb_vertically = int(np.sqrt(nb_img))\n",
    "    name_img_out = path+'visualize_img' + '_' + str(name) + '.png'\n",
    "    tls.visualize_grayscale_images(X[vector_viz, :, :, :],\n",
    "                                   nb_vertically, name_img_out)\n",
    "\n",
    "    # Labels\n",
    "    name_lab_out = path+'visualize_labels' + '_' + str(name)\n",
    "    labels = train_lab_new[vector_viz]\n",
    "    labels = labels.reshape((nb_vertically, nb_vertically))\n",
    "\n",
    "    file = open(name_lab_out, \"w\")\n",
    "    labels_str = ''\n",
    "    for line in labels:\n",
    "        for elt in line:\n",
    "            labels_str = labels_str + str(elt) + ' '\n",
    "        labels_str = labels_str + '\\n'\n",
    "    file.write(labels_str)\n",
    "    file.close()\n",
    "    print('Visualisation exported')\n",
    "\n",
    "\n",
    "path = os.getcwd() + '/img_observation/'\n",
    "X = train_img\n",
    "y = train_lab_new\n",
    "visualize(path, 'obs', X, y, nb_img=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the model within the file 'NeuralNetwork.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "from NeuralNetwork import neural_net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "nb_images = train_img_new.shape[0]\n",
    "nb_input = train_img_new.shape[1]\n",
    "batch_size = 1000\n",
    "nb_hidden = 16\n",
    "nb_classes = 10\n",
    "learning_rate = 0.5\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used in order to define batches during the training (it is the equivalent of the next batch function of tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_idx(i, batch_size, nb_images):\n",
    "    # Input\n",
    "    max_batch_size = int(nb_images/batch_size)\n",
    "    j = i % max_batch_size\n",
    "    return np.arange(j*batch_size, (j+1)*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "X = tf.placeholder(tf.float32, [None, nb_input])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "p = neural_net_model(X, nb_input, batch_size, nb_hidden, nb_classes,\n",
    "                     learning_rate)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(p),\n",
    "                                              reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(p, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "acc_train = []\n",
    "loss_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:38,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Loss : 0.7467897 Accuracy : 0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:08<00:33,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Loss : 0.6626388 Accuracy : 0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:12<00:29,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Loss : 0.5267002 Accuracy : 0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:16<00:25,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Loss : 0.5051738 Accuracy : 0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:21<00:21,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Loss : 0.48919243 Accuracy : 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:25<00:16,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Loss : 0.4758233 Accuracy : 0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:29<00:12,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Loss : 0.43951526 Accuracy : 0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:33<00:08,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Loss : 0.42350593 Accuracy : 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:37<00:04,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Loss : 0.3957326 Accuracy : 0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 10/10 [00:42<00:00,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Loss : 0.38083085 Accuracy : 0.893\n",
      "\n",
      "Accuracy on test set: 0.893\n",
      "\n",
      "Errors (%): 0.105 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        saver.restore(sess, 'mnist_predictions.ckpt')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # train the model mini batch with 100 elements, for 1K times\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            batch_index = get_batch_idx(i, batch_size, nb_images)\n",
    "            batch_x, batch_y = (train_img_new[batch_index, :],\n",
    "                                train_labels[batch_index])\n",
    "            sess.run([train_step], feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "        # Accuracy and cross entropy - Training phase\n",
    "        loss, acc = sess.run([cross_entropy, accuracy],\n",
    "                             feed_dict={X: valid_img_new, Y: valid_labels})\n",
    "        loss_train.append(loss)\n",
    "        acc_train.append(acc)\n",
    "        print(\"\\nEpoch\", str(epoch), \"Loss :\", str(loss),\n",
    "              \"Accuracy :\", str(acc))\n",
    "\n",
    "    # Test\n",
    "    # evaluate the accuracy of the model\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: test_img_new,\n",
    "                                                  Y: test_labels})\n",
    "    print(\"\\nAccuracy on test set:\", acc)\n",
    "\n",
    "    # pred\n",
    "    pred = sess.run(p, feed_dict={X: test_img_new, Y: test_labels})\n",
    "    pred = np.array([np.argmax(pred[y]) for y in range(len(pred))])\n",
    "    labels = np.array([np.argmax(test_labels[y])\n",
    "                       for y in range(len(test_labels))])\n",
    "\n",
    "    print(\"\\nErrors (%):\", np.sum(labels != pred)/len(pred), '%')\n",
    "\n",
    "    # Save the model\n",
    "    '''if input('Save model ? [Y/N]') == 'Y':\n",
    "        import os\n",
    "        saver.save(sess, os.getcwd() +\n",
    "                   '/nn_saved_sessions/mnist_predictions.ckpt')\n",
    "        print('Model Saved')'''\n",
    "\n",
    "    # Close the session\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), acc_train, label='accuracy', color='blue')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('accuracy - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entrepoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), loss_train, label='loss', color='red')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('loss - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Evolved Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNetwork import neural_net_model_evol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "nb_images = train_img_new.shape[0]\n",
    "nb_input = train_img_new.shape[1]\n",
    "batch_size = 1000\n",
    "nb_hidden1 = 64\n",
    "nb_hidden2 = 64\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# Drop out level\n",
    "prob_1 = 0.7\n",
    "prob_2 = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used in order to define batches during the training (it is the equivalent of the next batch function of tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_idx(i, batch_size, nb_images):\n",
    "    # Input\n",
    "    max_batch_size = int(nb_images/batch_size)\n",
    "    j = i % max_batch_size\n",
    "    return np.arange(j*batch_size, (j+1)*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "X = tf.placeholder(tf.float32, [None, nb_input])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "keep_prob_1 = tf.placeholder(tf.float32)\n",
    "keep_prob_2 = tf.placeholder(tf.float32)\n",
    "\n",
    "p = neural_net_model_evol(X, nb_input, nb_hidden1, nb_hidden2, nb_classes,\n",
    "                          keep_prob_1, keep_prob_2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(p),\n",
    "                                              reduction_indices=[1]))\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(p, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "learn_rate = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a piecewise constant for learning rate in order not to have a constant value through the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piecewise constant\n",
    "min_lr = 0.1\n",
    "max_lr = 0.5\n",
    "nb_values = 20\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "boundaries = list(np.linspace(batch_size,\n",
    "                              batch_size*nb_epoch, nb_values,\n",
    "                              dtype=np.int32)[:-1])\n",
    "values = list(np.round(np.linspace(max_lr, min_lr, nb_values), 2))\n",
    "learning_rate_pc = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "\n",
    "j = 0\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "learning_step_pc = tf.train.GradientDescentOptimizer(learning_rate_pc).minimize(cross_entropy,\n",
    "                                                                 global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:15<04:59, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Loss : 0.8074657 Accuracy : 0.78 Learning rate: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:29<04:33, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Loss : 0.61264664 Accuracy : 0.826 Learning rate: 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:43<04:11, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Loss : 0.51855874 Accuracy : 0.861 Learning rate: 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:57<03:52, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Loss : 0.46155068 Accuracy : 0.873 Learning rate: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [01:11<03:37, 14.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Loss : 0.41282997 Accuracy : 0.886 Learning rate: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [01:24<03:16, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Loss : 0.41816065 Accuracy : 0.875 Learning rate: 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [01:38<03:01, 13.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Loss : 0.37493274 Accuracy : 0.886 Learning rate: 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [01:52<02:48, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Loss : 0.38461548 Accuracy : 0.884 Learning rate: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [02:07<02:35, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Loss : 0.34910175 Accuracy : 0.894 Learning rate: 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [02:21<02:21, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Loss : 0.34067485 Accuracy : 0.894 Learning rate: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [02:35<02:06, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Loss : 0.3161504 Accuracy : 0.903 Learning rate: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [02:48<01:51, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 Loss : 0.32059976 Accuracy : 0.911 Learning rate: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [03:02<01:37, 13.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 Loss : 0.3279292 Accuracy : 0.896 Learning rate: 0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [03:16<01:22, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 Loss : 0.30745184 Accuracy : 0.909 Learning rate: 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [03:31<01:11, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Loss : 0.30020717 Accuracy : 0.904 Learning rate: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [03:47<00:58, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Loss : 0.29974905 Accuracy : 0.911 Learning rate: 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [04:01<00:43, 14.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 Loss : 0.28851065 Accuracy : 0.914 Learning rate: 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [04:16<00:29, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 Loss : 0.2945744 Accuracy : 0.911 Learning rate: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [04:28<00:14, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 Loss : 0.28958887 Accuracy : 0.909 Learning rate: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [04:42<00:00, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 Loss : 0.2905008 Accuracy : 0.912 Learning rate: 0.1\n",
      "\n",
      "Accuracy on test set: 0.912\n",
      "\n",
      "Errors (%): 8.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        saver.restore(sess, 'mnist_predictions.ckpt')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # train the model mini batch with 100 elements, for 1K times\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Update learning rate\n",
    "            l_rate = sess.run(learning_rate_pc, {global_step: j})\n",
    "            j += 1\n",
    "\n",
    "            batch_index = get_batch_idx(i, batch_size, nb_images)\n",
    "            batch_x, batch_y = (train_img_new[batch_index, :],\n",
    "                                train_labels[batch_index])\n",
    "            sess.run([learning_step_pc], feed_dict={X: batch_x, Y: batch_y,\n",
    "                                                    keep_prob_1: prob_1,\n",
    "                                                    keep_prob_2: prob_2})\n",
    "\n",
    "        # Accuracy and cross entropy - Training phase\n",
    "        loss, acc = sess.run([cross_entropy, accuracy],\n",
    "                             feed_dict={X: valid_img_new, Y: valid_labels,\n",
    "                                        keep_prob_1: 1.0,\n",
    "                                        keep_prob_2: 1.0})\n",
    "        loss_train.append(loss)\n",
    "        acc_train.append(acc)\n",
    "        learn_rate.append(l_rate)\n",
    "        print(\"\\nEpoch\", str(epoch), \"Loss :\", str(loss),\n",
    "              \"Accuracy :\", str(acc), \"Learning rate:\", l_rate)\n",
    "\n",
    "    # Test\n",
    "    # evaluate the accuracy of the model\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: test_img_new,\n",
    "                                                  Y: test_labels,\n",
    "                                                  keep_prob_1: 1.0,\n",
    "                                                  keep_prob_2: 1.0})\n",
    "    print(\"\\nAccuracy on test set:\", acc)\n",
    "\n",
    "    # pred\n",
    "    pred = sess.run(p, feed_dict={X: test_img_new, Y: test_labels,\n",
    "                                  keep_prob_1: 1.0, keep_prob_2: 1.0})\n",
    "    pred = np.array([np.argmax(pred[y]) for y in range(len(pred))])\n",
    "    labels = np.array([np.argmax(test_labels[y])\n",
    "                       for y in range(len(test_labels))])\n",
    "\n",
    "    print(\"\\nErrors (%):\", np.sum(labels != pred)/len(pred)*100, '%')\n",
    "\n",
    "    # Save the model\n",
    "    '''if input('Save model ? [Y/N]') == 'Y':\n",
    "        import os\n",
    "        saver.save(sess, os.getcwd() +\n",
    "                   '/nn_saved_sessions/mnist_predictions.ckpt')\n",
    "        print('Model Saved')'''\n",
    "\n",
    "    # Close the session\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), acc_train, label='accuracy', color='blue')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('accuracy - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), loss_train, label='loss', color='red')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('loss - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the learning rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), learn_rate, label='learning_rate', color='green')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('learning rate - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now use a exponential decay for the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = []\n",
    "loss_train = []\n",
    "learn_rate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential decay of the learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.7\n",
    "learning_rate_ed = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                              global_step, batch_size, 0.69,\n",
    "                                              staircase=True)\n",
    "j = 0\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "learning_step_ed = tf.train.GradientDescentOptimizer(learning_rate_ed).minimize(cross_entropy,\n",
    "                                                   global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:07<02:18,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Loss : 0.72139025 Accuracy : 0.783 Learning rate: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:14<02:11,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Loss : 0.57925904 Accuracy : 0.818 Learning rate: 0.48299998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [00:21<02:03,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Loss : 0.4916994 Accuracy : 0.861 Learning rate: 0.33326998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [00:29<01:56,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Loss : 0.45890844 Accuracy : 0.869 Learning rate: 0.2299563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [00:36<01:48,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Loss : 0.44532216 Accuracy : 0.872 Learning rate: 0.15866984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [00:43<01:41,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Loss : 0.42917398 Accuracy : 0.876 Learning rate: 0.1094822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [00:50<01:34,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Loss : 0.4316134 Accuracy : 0.876 Learning rate: 0.07554271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [00:57<01:26,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Loss : 0.41762635 Accuracy : 0.882 Learning rate: 0.05212447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [01:05<01:19,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Loss : 0.41163066 Accuracy : 0.885 Learning rate: 0.035965886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [01:12<01:12,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Loss : 0.41399896 Accuracy : 0.884 Learning rate: 0.02481646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [01:19<01:05,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Loss : 0.409713 Accuracy : 0.883 Learning rate: 0.017123358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [01:26<00:57,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 Loss : 0.41029066 Accuracy : 0.879 Learning rate: 0.011815117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [01:34<00:50,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 Loss : 0.40891463 Accuracy : 0.88 Learning rate: 0.008152431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [01:41<00:43,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 Loss : 0.40875635 Accuracy : 0.881 Learning rate: 0.0056251767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [01:48<00:36,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 Loss : 0.40814728 Accuracy : 0.881 Learning rate: 0.0038813723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [01:55<00:28,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 Loss : 0.40851933 Accuracy : 0.879 Learning rate: 0.0026781466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [02:03<00:21,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 Loss : 0.4088267 Accuracy : 0.88 Learning rate: 0.0018479213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [02:10<00:14,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 Loss : 0.40871632 Accuracy : 0.88 Learning rate: 0.0012750656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [02:17<00:07,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 Loss : 0.40873408 Accuracy : 0.88 Learning rate: 0.0008797953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 Loss : 0.40874663 Accuracy : 0.879 Learning rate: 0.00060705876\n",
      "\n",
      "Accuracy on test set: 0.879\n",
      "\n",
      "Errors (%): 11.87 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize variables and session\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        saver.restore(sess, 'mnist_predictions.ckpt')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # train the model mini batch with 100 elements, for 1K times\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Update learning rate\n",
    "            l_rate = sess.run(learning_rate_ed, {global_step: j})\n",
    "            j += 1\n",
    "\n",
    "            batch_index = get_batch_idx(i, batch_size, nb_images)\n",
    "            batch_x, batch_y = (train_img_new[batch_index, :],\n",
    "                                train_labels[batch_index])\n",
    "            sess.run([learning_step_ed], feed_dict={X: batch_x, Y: batch_y,\n",
    "                                                    keep_prob_1: prob_1,\n",
    "                                                    keep_prob_2: prob_2})\n",
    "\n",
    "        # Accuracy and cross entropy - Training phase\n",
    "        loss, acc = sess.run([cross_entropy, accuracy],\n",
    "                             feed_dict={X: valid_img_new, Y: valid_labels,\n",
    "                                        keep_prob_1: 1.0,\n",
    "                                        keep_prob_2: 1.0})\n",
    "        loss_train.append(loss)\n",
    "        acc_train.append(acc)\n",
    "        learn_rate.append(l_rate)\n",
    "        print(\"\\nEpoch\", str(epoch), \"Loss :\", str(loss),\n",
    "              \"Accuracy :\", str(acc), \"Learning rate:\", l_rate)\n",
    "\n",
    "    # Test\n",
    "    # evaluate the accuracy of the model\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: test_img_new,\n",
    "                                                  Y: test_labels,\n",
    "                                                  keep_prob_1: 1.0,\n",
    "                                                  keep_prob_2: 1.0})\n",
    "    print(\"\\nAccuracy on test set:\", acc)\n",
    "\n",
    "    # pred\n",
    "    pred = sess.run(p, feed_dict={X: test_img_new, Y: test_labels,\n",
    "                                  keep_prob_1: 1.0, keep_prob_2: 1.0})\n",
    "    pred = np.array([np.argmax(pred[y]) for y in range(len(pred))])\n",
    "    labels = np.array([np.argmax(test_labels[y])\n",
    "                       for y in range(len(test_labels))])\n",
    "\n",
    "    print(\"\\nErrors (%):\", np.sum(labels != pred)/len(pred)*100, '%')\n",
    "\n",
    "    # Save the model\n",
    "    '''if input('Save model ? [Y/N]') == 'Y':\n",
    "        import os\n",
    "        saver.save(sess, os.getcwd() +\n",
    "                   '/nn_saved_sessions/mnist_predictions.ckpt')\n",
    "        print('Model Saved')'''\n",
    "\n",
    "    # Close the session\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (40,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-14c89955596d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualisation of the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training phase'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy - training phase'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m     return gca().plot(\n\u001b[0;32m-> 2749\u001b[0;31m         *args, scalex=scalex, scaley=scaley, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   2750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2751\u001b[0m \u001b[0;31m# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1785\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (40,)"
     ]
    }
   ],
   "source": [
    "# Visualisation of the accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), acc_train, label='accuracy', color='blue')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('accuracy - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), loss_train, label='loss', color='red')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('loss - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the learning rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(nb_epoch), learn_rate, label='learning_rate', color='green')\n",
    "plt.title('Training phase')\n",
    "plt.ylabel('learning rate - training phase')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
